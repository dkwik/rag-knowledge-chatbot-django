{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Articles\n",
    "\n",
    "-https://medium.com/@sauravjoshi23/complex-query-resolution-through-llamaindex-utilizing-recursive-retrieval-document-agents-and-sub-d4861ecd54e6\n",
    "\n",
    "-https://medium.com/@sauravjoshi23/building-knowledge-graphs-rebel-llamaindex-and-rebel-llamaindex-8769cf800115#:~:text=language%20for%20NebulaGraph.-,Relation%20Extraction%20By%20End%2Dto%2Dend%20Language%20generation%20(REBEL,filtered%20with%20a%20RoBERTa%20model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load environment variables from .env file\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import psycopg2\n",
    "from decouple import config\n",
    "from sqlalchemy.engine import make_url\n",
    "from transformers import pipeline\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    KnowledgeGraphIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.vector_stores import PGVectorStore\n",
    "from llama_index.graph_stores import Neo4jGraphStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load documents\n",
    "titles = [\n",
    "    \"DeloitteFutureOfAI\"\n",
    "    ]\n",
    "\n",
    "documents = {}\n",
    "for title in titles:\n",
    "    documents[title] = SimpleDirectoryReader(input_files=[f\"data/{title}.pdf\"]).load_data()\n",
    "print(f\"loaded documents with {len(documents)} documents\")\n",
    "\n",
    "#load llm\n",
    "OPENAI_API_KEY = config('OPENAI_API_KEY')\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize vectorstore config\n",
    "connection_string = config('PGVECTOR_CONNECTION_STRING')\n",
    "db_name = config('PGVECTOR_DATABASE')\n",
    "conn = psycopg2.connect(connection_string)\n",
    "conn.autocommit = True\n",
    "\n",
    "\n",
    "# construct vector store and customize storage context\n",
    "url = make_url(connection_string)\n",
    "\n",
    "vector_storage_context = StorageContext.from_defaults(\n",
    "    vector_store = PGVectorStore.from_params(\n",
    "        database=db_name,\n",
    "        host=url.host,\n",
    "        password=url.password,\n",
    "        port=url.port,\n",
    "        user=url.username,\n",
    "        table_name=\"DeloitteFutureOfAI\",\n",
    "        embed_dim=1536,  # openai embedding dimension\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "\n",
    "# Build agents dictionary\n",
    "agents = {}\n",
    "\n",
    "for title in titles:\n",
    "    # build vector index\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents[title], service_context=service_context\n",
    "    )\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex.from_documents(\n",
    "        documents[title], service_context=service_context\n",
    "    )\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "    list_query_engine = summary_index.as_query_engine()\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"vector_tool\",\n",
    "                description=(\n",
    "                    \"Useful for summarization questions related to\"\n",
    "                    f\" {title}\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=list_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"summary_tool\",\n",
    "                description=(\n",
    "                    f\"Useful for retrieving specific context from {title}\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    agents[title] = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define top-level nodes\n",
    "nodes = []\n",
    "for title in titles:\n",
    "    # define index node that links to these agents\n",
    "    title_summary = (\n",
    "        f\"This content contains Wikipedia articles about {title}. Use\"\n",
    "        \" this index if you need to lookup specific facts about\"\n",
    "        f\" {title}.\\nDo not use this index if you want to analyze\"\n",
    "        \" multiple cities.\"\n",
    "    )\n",
    "    node = IndexNode(text=title_summary, index_id=title)\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define top-level retriever\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "# define recursive retriever\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "# note: can pass `agents` dict as `query_engine_dict` since every agent can be used as a query engine\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever},\n",
    "    query_engine_dict=agents,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    # service_context=service_context,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    recursive_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either way we can now query the index\n",
    "response = query_engine.query(\"What can we liken to C-3PO and Chewbacca?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
