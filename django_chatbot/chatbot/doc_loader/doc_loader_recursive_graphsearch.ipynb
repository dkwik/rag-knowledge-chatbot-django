{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Articles\n",
    "\n",
    "-https://medium.com/@sauravjoshi23/complex-query-resolution-through-llamaindex-utilizing-recursive-retrieval-document-agents-and-sub-d4861ecd54e6\n",
    "\n",
    "-https://medium.com/@sauravjoshi23/building-knowledge-graphs-rebel-llamaindex-and-rebel-llamaindex-8769cf800115#:~:text=language%20for%20NebulaGraph.-,Relation%20Extraction%20By%20End%2Dto%2Dend%20Language%20generation%20(REBEL,filtered%20with%20a%20RoBERTa%20model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load environment variables from .env file\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import psycopg2\n",
    "from decouple import config\n",
    "from sqlalchemy.engine import make_url\n",
    "from transformers import pipeline\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    KnowledgeGraphIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.vector_stores import PGVectorStore\n",
    "from llama_index.graph_stores import Neo4jGraphStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load documents\n",
    "titles = [\n",
    "    \"DeloitteFutureOfAI\"\n",
    "    ]\n",
    "\n",
    "documents = {}\n",
    "for title in titles:\n",
    "    documents[title] = SimpleDirectoryReader(input_files=[f\"data/{title}.pdf\"]).load_data()\n",
    "print(f\"loaded documents with {len(documents)} documents\")\n",
    "\n",
    "#load llm\n",
    "OPENAI_API_KEY = config('OPENAI_API_KEY')\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize vectorstore config\n",
    "connection_string = config('PGVECTOR_CONNECTION_STRING')\n",
    "db_name = config('PGVECTOR_DATABASE')\n",
    "conn = psycopg2.connect(connection_string)\n",
    "conn.autocommit = True\n",
    "\n",
    "\n",
    "# construct vector store and customize storage context\n",
    "url = make_url(connection_string)\n",
    "\n",
    "vector_storage_context = StorageContext.from_defaults(\n",
    "    vector_store = PGVectorStore.from_params(\n",
    "        database=db_name,\n",
    "        host=url.host,\n",
    "        password=url.password,\n",
    "        port=url.port,\n",
    "        user=url.username,\n",
    "        table_name=\"DeloitteTechTrends2024\",\n",
    "        embed_dim=1536,  # openai embedding dimension\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "triplet_extractor = pipeline(\n",
    "    'text2text-generation',\n",
    "    model='Babelscape/rebel-large',\n",
    "    tokenizer='Babelscape/rebel-large',\n",
    "    device=device)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_triplets(input_text, triplets):\n",
    "    \"\"\"Sometimes the model hallucinates, so we filter out entities\n",
    "       not present in the text\"\"\"\n",
    "    text = input_text.lower()\n",
    "    clean_triplets = []\n",
    "    for triplet in triplets:\n",
    "\n",
    "        if (triplet[\"head\"] == triplet[\"tail\"]):\n",
    "            continue\n",
    "\n",
    "        head_match = re.search(\n",
    "            r'\\b' + re.escape(triplet[\"head\"].lower()) + r'\\b', text)\n",
    "        if head_match:\n",
    "            head_index = head_match.start()\n",
    "        else:\n",
    "            head_index = text.find(triplet[\"head\"].lower())\n",
    "\n",
    "        tail_match = re.search(\n",
    "            r'\\b' + re.escape(triplet[\"tail\"].lower()) + r'\\b', text)\n",
    "        if tail_match:\n",
    "            tail_index = tail_match.start()\n",
    "        else:\n",
    "            tail_index = text.find(triplet[\"tail\"].lower())\n",
    "\n",
    "        if ((head_index == -1) or (tail_index == -1)):\n",
    "            continue\n",
    "\n",
    "        clean_triplets.append((triplet[\"head\"], triplet[\"type\"], triplet[\"tail\"]))\n",
    "\n",
    "    return clean_triplets\n",
    "\n",
    "def extract_triplets(input_text):\n",
    "    text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(input_text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])[0]\n",
    "\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail':object_.strip()})\n",
    "    clean = clean_triplets(input_text, triplets)\n",
    "    return clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD')\n",
    "os.environ[\"NEO4J_DB\"] = os.getenv('NEO4J_DB')\n",
    "\n",
    "graph_store = Neo4jGraphStore(\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "    url=os.environ[\"NEO4J_URI\"],\n",
    "    database=os.environ[\"NEO4J_DB\"],\n",
    ")\n",
    "\n",
    "graph_storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "from llama_index.agent import OpenAIAgent\n",
    "\n",
    "# Build agents dictionary\n",
    "agents = {}\n",
    "\n",
    "for doc_title in titles:\n",
    "    # build vector index\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents[doc_title], service_context=service_context, storage_context=vector_storage_context\n",
    "    )\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex.from_documents(\n",
    "        documents[doc_title], service_context=service_context\n",
    "    )\n",
    "    # build graph index\n",
    "    graph_index = KnowledgeGraphIndex.from_documents(\n",
    "        documents[doc_title],\n",
    "        storage_context=graph_storage_context,\n",
    "        kg_triplet_extract_fn=extract_triplets,\n",
    "        service_context=ServiceContext.from_defaults(llm=llm, chunk_size=256)\n",
    "    )\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "    list_query_engine = summary_index.as_query_engine()\n",
    "    graph_query_engine = graph_index.as_query_engine()\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"vector_tool\",\n",
    "                description=f\"Useful for retrieving specific context from {doc_title}\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=list_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"summary_tool\",\n",
    "                description=f\"Useful for summarization questions related to {doc_title}\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=graph_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"graph_tool\",\n",
    "                description=f\"Useful for retrieving structural, interconnected and relational knowledge related to {doc_title}\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    agents[doc_title] = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What can we liken to C-3PO and Chewbacca?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
